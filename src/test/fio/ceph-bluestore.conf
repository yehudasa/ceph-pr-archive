# example configuration file for ceph-bluestore.fio

[global]
	debug bluestore = 0/0
	debug bluefs = 0/0
	debug bdev = 0/0
	debug rocksdb = 0/0
	# spread objects over 8 collections
	osd pool default pg num = 8
	# increasing shards can help when scaling number of collections
	osd op num shards = 5

[osd]
	osd objectstore = bluestore
	bluestore rocksdb options = "max_bytes_for_level_base=524288,compression=kNoCompression,max_write_buffer_number=16,min_write_buffer_number_to_merge=2,recycle_log_file_num=16,compaction_threads=32,flusher_threads=8,max_background_compactions=32,max_background_flushes=8,write_buffer_size=83886080,level0_file_num_compaction_trigger=4,level0_slowdown_writes_trigger=400,level0_stop_writes_trigger=800,disableWAL=false"

	bluestore block create = true
	bluestore block path = /tmp/mogeb/nvme0

	bluestore block db path=/tmp/mogeb/nvme1
	bluestore block wal path=/tmp/mogeb/nvme2
	bluestore wal size = 100000
	# use directory= option from fio job file
	osd data = ${fio_dir}

	# log inside fio_dir
	log file = ${fio_dir}/log
